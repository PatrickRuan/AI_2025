{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PatrickRuan/AI_2025/blob/main/RAG_OpenAI/05_question_answering_OpenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YUi0sWD1oE0L"
      },
      "id": "YUi0sWD1oE0L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mphkMUjRoHQh",
        "outputId": "c6fb5041-5f2c-464a-8c54-192604f96e05"
      },
      "id": "mphkMUjRoHQh",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r drive/MyDrive/z-發布/\"70 單元_2025H1\"/\"AI 課程發佈\"/週三/RAG_OpenAI/docs ."
      ],
      "metadata": {
        "id": "AQ4-XKljoNKS"
      },
      "id": "AQ4-XKljoNKS",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4ed7feed",
      "metadata": {
        "id": "4ed7feed"
      },
      "source": [
        "# Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bef480fe",
      "metadata": {
        "id": "bef480fe"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Recall the overall workflow for retrieval augmented generation (RAG):"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07fa2dab",
      "metadata": {
        "id": "07fa2dab"
      },
      "source": [
        "![overview.jpeg](attachment:overview.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8825752",
      "metadata": {
        "id": "e8825752"
      },
      "source": [
        "We discussed `Document Loading` and `Splitting` as well as `Storage` and `Retrieval`.\n",
        "\n",
        "Let's load our vectorDB."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install dotenv\n",
        "#!pip install -U langchain-community\n",
        "#!pip install chromadb\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsgZrq11pVJ6",
        "outputId": "52834290-c56a-4532-de01-29fc8ffc3568"
      },
      "id": "NsgZrq11pVJ6",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b997cff1",
      "metadata": {
        "height": 166,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b997cff1",
        "outputId": "cca0e667-3cb5-4b3b-ecec-f9710ca768bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "請輸入你的 OpenAI API 金鑰：··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "import sys\n",
        "from getpass import getpass\n",
        "sys.path.append('../..')\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "#openai.api_key  = os.environ['OPENAI_API_KEY']\n",
        "\n",
        "openai.api_key = getpass(\"請輸入你的 OpenAI API 金鑰：\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc655b16-b832-4777-a651-2bb99c69b62e",
      "metadata": {
        "id": "dc655b16-b832-4777-a651-2bb99c69b62e"
      },
      "source": [
        "The code below was added to assign the openai LLM version filmed until it is deprecated, currently in Sept 2023.\n",
        "LLM responses can often vary, but the responses may be significantly different when using a different model version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ee60ebc6-b47c-4c04-af4e-620f161d5c00",
      "metadata": {
        "height": 132,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee60ebc6-b47c-4c04-af4e-620f161d5c00",
        "outputId": "a817062c-fe5e-4d48-b10e-6db9c4f2aa72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt-3.5-turbo\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "current_date = datetime.datetime.now().date()\n",
        "if current_date < datetime.date(2023, 9, 2):\n",
        "    llm_name = \"gpt-3.5-turbo-0301\"\n",
        "else:\n",
        "    llm_name = \"gpt-3.5-turbo\"\n",
        "\n",
        "print(llm_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e63e5f31",
      "metadata": {
        "height": 113,
        "tags": [],
        "id": "e63e5f31"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "persist_directory = 'docs/chroma/'\n",
        "#embedding = OpenAIEmbeddings(openai_api_key=openai.api_key)\n",
        "\n",
        "embedding = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    openai_api_key=openai.api_key\n",
        ")\n",
        "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "15fd1341",
      "metadata": {
        "height": 30,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15fd1341",
        "outputId": "d263ed03-9851-4a86-ee28-f6ca79d11a20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "208\n"
          ]
        }
      ],
      "source": [
        "print(vectordb._collection.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3a689f25",
      "metadata": {
        "height": 64,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a689f25",
        "outputId": "37ca8d73-d881-4395-dc45-7820d2cc3e1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# 這裡會出錯，因為 openai 用的 default embedding text 已經不一樣...\n",
        "\n",
        "question = \"What are major topics for this class?\"\n",
        "docs = vectordb.similarity_search(question,k=3)\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "c67dad34",
      "metadata": {
        "height": 47,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c67dad34",
        "outputId": "549a9483-89e8-42a0-d9eb-a095573d2077"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-c14ad9c02e56>:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(model_name=llm_name, temperature=0, openai_api_key=openai.api_key)\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "llm = ChatOpenAI(model_name=llm_name, temperature=0, openai_api_key=openai.api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37ba2ad8",
      "metadata": {
        "id": "37ba2ad8"
      },
      "source": [
        "### RetrievalQA chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "5b3ebcdd",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "5b3ebcdd"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "41f0003f",
      "metadata": {
        "height": 81,
        "id": "41f0003f"
      },
      "outputs": [],
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "aac0334e",
      "metadata": {
        "height": 30,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aac0334e",
        "outputId": "c10787b8-0865-4e54-8820-8e6c65ab55b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-ef85128c39b8>:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = qa_chain({\"query\": question})\n"
          ]
        }
      ],
      "source": [
        "result = qa_chain({\"query\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "10227125",
      "metadata": {
        "height": 30,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "10227125",
        "outputId": "8882ff15-0f0d-40b6-f2ef-ce6242c90cf4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The major topics covered in this class include linear regression, logistic regression, regularization techniques, neural networks, support vector machines, clustering algorithms, dimensionality reduction, anomaly detection, recommender systems, and large-scale machine learning.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fae8e55",
      "metadata": {
        "id": "5fae8e55"
      },
      "source": [
        "### Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "2b37519f",
      "metadata": {
        "height": 181,
        "tags": [],
        "id": "2b37519f"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Build prompt\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "fcb5817c",
      "metadata": {
        "height": 132,
        "id": "fcb5817c"
      },
      "outputs": [],
      "source": [
        "# Run chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "e0fd6824",
      "metadata": {
        "height": 30,
        "id": "e0fd6824"
      },
      "outputs": [],
      "source": [
        "question = \"Is probability a class topic?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "f3a21b52",
      "metadata": {
        "height": 30,
        "id": "f3a21b52"
      },
      "outputs": [],
      "source": [
        "result = qa_chain({\"query\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "74e2f6cc",
      "metadata": {
        "height": 30,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "74e2f6cc",
        "outputId": "f0b8e0f5-ba23-48c8-bdce-eb212c17cb36"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I don't know, thanks for asking!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "4a2531ba",
      "metadata": {
        "height": 30,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a2531ba",
        "outputId": "1d9c83bf-1fb8-41ef-d369-1d351908ae31"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'total_pages': 18, 'page': 3, 'creator': 'PScript5.dll Version 5.2.2', 'source': './MachineLearning-Lecture02.pdf', 'author': '', 'page_label': '4', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'title': '', 'moddate': '2008-07-11T11:25:05-07:00', 'creationdate': '2008-07-11T11:25:05-07:00'}, page_content=\"may have more than one input feature. So for example, if instead of just knowing the size \\nof the houses, if we know also the number of bedrooms in these houses, let's say, then, so \\nif our training set also has a second feature, the number of bedrooms in the house, then \\nyou may, let's say X1 denote the size and square feet. Let X have script two denote the \\nnumber of bedrooms, and then I would write the hypothesis, H of X, as theta rho plus \\ntheta 1X1 plus theta 2X2.  \\nOkay, and sometimes when I went to take the hypothesis H, and when I went to make \\nthis dependent on the theta is explicit, I'll sometimes write this as H subscript theta of X. \\nAnd so this is the price that my hypothesis predicts a house with features X costs. So \\ngiven the new house with features X, a certain size and a certain number of bedrooms, \\nthis is going to be the price that my hypothesis predicts this house is going to cost.  \\nOne final piece of notation, so for conciseness, just to write this a bit more compactly I'm \\ngoing to take the convention of defining X0 to be equal to one, and so I can now write H \\nof X to be equal to sum from I equals one to two of theta I, oh sorry, zero to two, theta I, \\nX I. And if you think of theta as an X, as vectors, then this is just theta transpose X.  \\nAnd the very final piece of notation is I'm also going to let lower case N be the number of \\nfeatures in my learning problem. And so this actually becomes a sum from I equals zero\")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "result[\"source_documents\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d71561c9",
      "metadata": {
        "id": "d71561c9"
      },
      "source": [
        "### RetrievalQA chain types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "baed8815",
      "metadata": {
        "height": 98,
        "id": "baed8815"
      },
      "outputs": [],
      "source": [
        "qa_chain_mr = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    chain_type=\"map_reduce\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "5c92c614",
      "metadata": {
        "height": 30,
        "id": "5c92c614"
      },
      "outputs": [],
      "source": [
        "result = qa_chain_mr({\"query\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "1ab98d11",
      "metadata": {
        "height": 30,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "1ab98d11",
        "outputId": "0ab21a0a-a813-4c8c-caaf-be1cdac1d00c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the extracted parts of the document provided, probability does not appear to be a class topic. The focus seems to be on regression techniques and machine learning algorithms for predicting house prices based on features.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a6a4633",
      "metadata": {
        "id": "0a6a4633"
      },
      "source": [
        "If you wish to experiment on the `LangSmith platform` (previously known as LangChain Plus):\n",
        "\n",
        " * Go to [LangSmith](https://www.langchain.com/langsmith) and sign up\n",
        " * Create an API key from your account's settings\n",
        " * Use this API key in the code below   \n",
        " * uncomment the code  \n",
        " Note, the endpoint in the video differs from the one below. Use the one below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d8c92ee",
      "metadata": {
        "height": 96,
        "tags": [],
        "id": "4d8c92ee"
      },
      "outputs": [],
      "source": [
        "#import os\n",
        "#os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "#os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
        "#os.environ[\"LANGCHAIN_API_KEY\"] = \"...\" # replace dots with your api key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "d6c74286",
      "metadata": {
        "height": 132,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "d6c74286",
        "outputId": "f77307ae-7eda-4d9e-9619-960cb9c43d30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the extracted parts of the document provided, probability is not explicitly mentioned as a class topic in the context of locally weighted linear regression, gradient descent, and linear regression for housing prices data. Therefore, it is unclear from the given information whether probability is a class topic in this specific context.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "qa_chain_mr = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    chain_type=\"map_reduce\"\n",
        ")\n",
        "result = qa_chain_mr({\"query\": question})\n",
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "26cd056a",
      "metadata": {
        "height": 132,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "26cd056a",
        "outputId": "a99c3f2e-e058-43ca-ae2d-e68ef7ee1c18"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The discussion is focused on locally weighted linear regression in the context of machine learning, specifically on the topic of gradient descent and convergence to a local minimum. Probability may not be the main topic of discussion in this particular context. However, probability is a fundamental concept in machine learning and is often covered in machine learning courses as it forms the basis for many algorithms and models. In the specific example provided, the algorithm of locally weighted regression is described as a non-parametric learning algorithm that allows for less reliance on carefully choosing features. This algorithm can be used without the need to manually select and adjust features, making it a valuable tool in machine learning applications. If you have specific questions about probability in the context of machine learning or any other topic, feel free to ask!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "qa_chain_mr = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    chain_type=\"refine\"\n",
        ")\n",
        "result = qa_chain_mr({\"query\": question})\n",
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90e7f14a",
      "metadata": {
        "id": "90e7f14a"
      },
      "source": [
        "### RetrievalQA limitations\n",
        "\n",
        "QA fails to preserve conversational history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b82d0019",
      "metadata": {
        "height": 81,
        "id": "b82d0019"
      },
      "outputs": [],
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectordb.as_retriever()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e469441a",
      "metadata": {
        "height": 64,
        "tags": [],
        "id": "e469441a"
      },
      "outputs": [],
      "source": [
        "question = \"Is probability a class topic?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a56ab1f",
      "metadata": {
        "height": 64,
        "tags": [],
        "id": "4a56ab1f"
      },
      "outputs": [],
      "source": [
        "question = \"why are those prerequesites needed?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6b6ad42-17d6-4acf-b98d-d444e883cdc8",
      "metadata": {
        "id": "c6b6ad42-17d6-4acf-b98d-d444e883cdc8"
      },
      "source": [
        "Note, The LLM response varies. Some responses **do** include a reference to probability which might be gleaned from referenced documents. The point is simply that the model does not have access to past questions or answers, this will be covered in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37053d31-77e7-4bac-81ad-5cfe337e1f80",
      "metadata": {
        "height": 30,
        "id": "37053d31-77e7-4bac-81ad-5cfe337e1f80"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3_11_7",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}